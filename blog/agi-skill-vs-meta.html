<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Insight #3 – AGI is not one thing, but two | Aleksey Bykhun</title>
  <style type="text/css">
  body {
    background: url(../smoke-pixel.gif);
    font-size: 20px;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
  }
  ::selection {
    background: #faec82;
  }
  article {
    background: rgba(255,255,255,0.9);
    padding: 30px;
  }
  a {
    background: white;
  }
  .date {
    color: #666;
    font-style: italic;
  }
  h1 {
    margin-top: 0;
  }
  .back {
    display: inline-block;
    margin-bottom: 20px;
  }
  </style>
</head>
<body>
  <a href="index.html" class="back">← back to blog</a>
  <article>
    <p class="date">January 15, 2026</p>
    <h1>Insight #3 – AGI is not one thing, but two</h1>
    
    <p>There's <strong>Skill AGI</strong>, and there's <strong>Meta AGI</strong> (no pun intended)</p>
    
    <p><strong>Skill AGI</strong> is the AI that's being trained any human skill into it – from coding and protein folding to poetry, music composition, chess. This is the "spinal brain"</p>
    
    <p><strong>Meta AGI</strong> is the AI that can learn new skills efficiently, with minimal examples – the way humans do. This is the "prefrontal cortex"</p>
    
    <p>This Skill AGI can get to ASI very quick – within years, not decades. I would argue that LLMs are already superhuman at what they do: pattern matching across vast knowledge, generating coherent text, holding context across long conversations.</p>
    
    <p>The funny thing is that Skill AGI alone might be enough to transform economy. We might not need anything else except for throwing more data into current transformer architecture.</p>
    
    <p>But here's the question: <strong>what are current LLMs much worse than humans at?</strong></p>
    
    <p><strong>Learning.</strong> No human needs 1000s of examples to understand a thing. Usually it's enough to read three good examples, and then practice for 40 hours – and you're good.</p>
    
    <p>Current AI is nowhere close to that. Self-created RL gyms are the best shot – that's pretty close to what our brains are doing when presented with new information – they simulate and collide it with existing concepts, resolving the "cognitive dissonance"</p>
    
    <p>However, here's the contrarian take: <strong>we might not even need Meta AGI at all.</strong></p>
    
    <p>Naive scaling tells us that if you could plug Opus 4.5 directly into economy, and then RL it on the $$$ amount it makes, it would be forced to develop sample-efficient learning as a subskill – because learning fast = making money faster. The selection pressure would create the meta-capability automatically. No separate "meta-learner" architecture is needed.</p>
    
    <p><strong>Three possible futures:</strong></p>
    <ol>
      <li>Skill AGI alone transforms everything, Meta AGI turns out to be unnecessary</li>
      <li>We build Meta AGI explicitly through better architectures</li>
      <li>Meta AGI emerges implicitly from economic selection pressure</li>
    </ol>
    
    <p>I'm betting on some combination of 1 and 3.</p>
  </article>
</body>
</html>
